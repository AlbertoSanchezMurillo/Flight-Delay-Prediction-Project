version: "3.8"

services:

  mongo:
    image: mongo:7.0.17
    container_name: mongo
    ports:
      - "27017:27017"
    volumes:
      - ./data_mongo:/data/db
    networks:
      - hdfs

  mongo-seed:
    image: mongo:7.0.17
    container_name: mongo_seed
    depends_on:
      - mongo
    volumes:
      - ./data:/data
    command: bash -c " sleep 5 &&
      mongoimport --host mongo --port 27017 \
      --db agile_data_science --collection origin_dest_distances \
      --file /data/origin_dest_distances.jsonl --type json"
    networks:
      - hdfs

  flaskapp:
    build:
      context: .
      dockerfile: resources/web/Dockerfile
    container_name: flaskapp
    ports:
      - "5010:5010"
    environment:
      - FLASK_ENV=development
      - MONGO_URI=mongodb://mongo:27017
      - PROJECT_HOME=/ap
    depends_on:
      - mongo
      - mongo-seed
      - kafka
    networks:
      - hdfs

  kafka:
    image: bitnami/kafka:3.9
    container_name: kafka
    ports:
      - "9092:9092"
      - "9094:9094"
    environment:
      - KAFKA_KRAFT_MODE=true
      - KAFKA_CFG_NODE_ID=1
      - KAFKA_CFG_PROCESS_ROLES=controller,broker
      - KAFKA_CFG_CONTROLLER_QUORUM_VOTERS=1@kafka:9093
      - KAFKA_CFG_LISTENERS=PLAINTEXT://:9092,CONTROLLER://:9093
      - KAFKA_CFG_ADVERTISED_LISTENERS=PLAINTEXT://kafka:9092
      - KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP=CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT
      - KAFKA_CFG_CONTROLLER_LISTENER_NAMES=CONTROLLER
      - KAFKA_CFG_AUTO_CREATE_TOPICS_ENABLE=true
      - KAFKA_CFG_LOG_DIRS=/bitnami/kafka/data
    volumes:
      - kafka_data:/bitnami/kafka
    networks:
      - hdfs

  spark-master:
    image: bde2020/spark-master:3.2.1-hadoop3.2
    container_name: spark-master
    ports:
      - "7077:7077"
      - "8088:8080"
    environment:
      - INIT_DAEMON_STEP=setup_spark
    volumes:
      - ./models:/models/models
    networks:
      - hdfs

  spark-worker-1:
    image: bde2020/spark-worker:3.2.1-hadoop3.2
    container_name: spark-worker-1
    depends_on:
      - spark-master
    ports:
      - "8086:8081"
    environment:
      - SPARK_MASTER=spark://spark-master:7077
      - INIT_DAEMON_STEP=setup_spark
    volumes:
      - ./models:/models/models
    networks:
      - hdfs

  spark-worker-2:
    image: bde2020/spark-worker:3.2.1-hadoop3.2
    container_name: spark-worker-2
    depends_on:
      - spark-master
    ports:
      - "8087:8081"
    environment:
      - SPARK_MASTER=spark://spark-master:7077
      - INIT_DAEMON_STEP=setup_spark
    volumes:
      - ./models:/models/models
    networks:
      - hdfs

  spark-submit:
    image: bde2020/spark-submit:3.2.1-hadoop3.2
    container_name: spark-submit
    depends_on:
      - spark-master
      - spark-worker-1
      - spark-worker-2
      - kafka
      - mongo
    environment:
      - SPARK_MASTER=spark://spark-master:7077
    command: >
      bash -c "sleep 20 &&
      /spark/bin/spark-submit \
      --master spark://spark-master:7077 \
      --packages org.mongodb.spark:mongo-spark-connector_2.12:10.4.1,org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.1 \
      --class es.upm.dit.ging.predictor.MakePrediction \
      /app/target/scala-2.12/flight_prediction_2.12-0.1.jar"
    volumes:
      - ./flight_prediction:/app
      - ./models:/models/models
    networks:
      - hdfs

  dataloader:
    image: python:3.8-slim
    container_name: dataloader
    volumes:
      - .:/app
    working_dir: /app
    command: >
      bash -c "apt update &&
      apt install -y curl &&
      chmod +x resources/download_data.sh &&
      ./resources/download_data.sh"
    depends_on:
      - mongo
    networks:
      - hdfs

  nifi:
    image: apache/nifi:1.24.0
    container_name: nifi
    ports:
      - "8085:8080"
    environment:
      - NIFI_WEB_HTTP_PORT=8080
    depends_on:
      - kafka
    volumes:
      - ./nifi_output:/output
    networks:
      - hdfs

  hadoop-namenode:
    image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
    container_name: hadoop-namenode
    ports:
      - "50070:50070"
      - "9870:9870"
    environment:
      - CLUSTER_NAME=test
      - INIT_DAEMON_STEP=setup_hdfs
      - HDFS_CONF_dfs_webhdfs_enabled=true
      - HDFS_CONF_dfs_permissions_enabled=false
      - CORE_CONF_hadoop_proxyuser_hue_hosts=*
      - CORE_CONF_hadoop_proxyuser_hue_groups=*
    volumes:
      - ./volumes/namenode:/hadoop/dfs/name
    networks:
      - hdfs

  hadoop-datanode:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: hadoop-datanode
    depends_on:
      - hadoop-namenode
    ports:
      - "50075:50075"
      - "9864:9864"
    environment:
      - CORE_CONF_fs_defaultFS=hdfs://hadoop-namenode:8020
      - HDFS_CONF_dfs_webhdfs_enabled=true
    volumes:
      - ./volumes/datanode:/hadoop/dfs/data
    networks:
      - hdfs

  airflow:
    build:
      context: .
      dockerfile: Dockerfile.airflow
    container_name: airflow
    restart: always
    depends_on:
      - postgres
    ports:
      - "8089:8080"
    environment:
      - AIRFLOW__CORE__DAGS_FOLDER=/opt/airflow/resources
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__CORE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres:5432/airflow
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
      - AIRFLOW__WEBSERVER__EXPOSE_CONFIG=True
      - _AIRFLOW_WWW_USER_USERNAME=admin
      - _AIRFLOW_WWW_USER_PASSWORD=admin
    volumes:
      - ./resources:/opt/airflow/resources
      - ./resources/airflow/dags:/opt/airflow/dags
      - airflow_python_packages:/home/airflow/.local
      - ./mlruns:/tmp/mlruns
      - /home/a.smurillo/Descargas/practica_creativa2/data:/opt/airflow/data
    command: >
      bash -c "airflow db init && 
           airflow users create --username admin --password admin --firstname admin --lastname admin --role Admin --email admin@example.com &&
           airflow webserver & airflow scheduler"
    networks:
      - hdfs

  postgres:
    image: postgres:13
    container_name: postgres
    environment:
      - POSTGRES_USER=airflow
      - POSTGRES_PASSWORD=airflow
      - POSTGRES_DB=airflow
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
    networks:
      - hdfs

  mlflow:
    image: ghcr.io/mlflow/mlflow:v2.0.1
    container_name: mlflow
    ports:
      - "5000:5000"
    command: mlflow server --host 0.0.0.0  --default-artifact-root ./mlflow_new/artifacts 
    volumes:
      - ./mlflow_new:/mlflow_new
    networks:
      - hdfs
volumes:
  kafka_data:
  model_volume:
  hadoop_namenode:
  hadoop_datanode:
  airflow_data:
  postgres_data:
  airflow_python_packages:

networks:
  hdfs:

